from fastapi import FastAPI, HTTPException, Request
from pydantic import BaseModel
from fastapi.middleware.cors import CORSMiddleware
import faiss
import pickle
import numpy as np
import os
import time
import asyncio
import google.generativeai as genai
from duckduckgo_search import AsyncDDGS  # T√¨m ki·∫øm Web b·∫•t ƒë·ªìng b·ªô
from openai import AsyncOpenAI           # OpenAI b·∫•t ƒë·ªìng b·ªô

# ================= 1. C·∫§U H√åNH APP & KH√ìA API =================
app = FastAPI()

app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],
    allow_methods=["*"],
    allow_headers=["*"],
)

# --- L·∫•y API Keys t·ª´ bi·∫øn m√¥i tr∆∞·ªùng ---
# 1. Google Gemini Keys (Danh s√°ch nhi·ªÅu key c√°ch nhau d·∫•u ph·∫©y)
GOOGLE_KEYS_STR = os.getenv("GOOGLE_API_KEYS", "")
GOOGLE_KEYS = [k.strip() for k in GOOGLE_KEYS_STR.split(",") if k.strip()]

# 2. OpenAI Key (D√πng ƒë·ªÉ Embed v√† l√†m Fallback)
OPENAI_API_KEY = os.getenv("OPENAI_API_KEY")
if not OPENAI_API_KEY:
    print("‚ö†Ô∏è C·∫¢NH B√ÅO: Thi·∫øu OPENAI_API_KEY. Ch·ª©c nƒÉng Search v√† Fallback s·∫Ω l·ªói.")

# Client OpenAI Async
openai_client = AsyncOpenAI(api_key=OPENAI_API_KEY) if OPENAI_API_KEY else None

# ================= 2. RATE LIMIT (CH·ªêNG SPAM) =================
RATE_LIMIT = {}
LIMIT = 10       # Cho ph√©p 10 requests
WINDOW = 60      # Trong 60 gi√¢y (1 ph√∫t)

def check_rate_limit(ip):
    now = time.time()
    # D·ªçn d·∫πp IP c≈©
    if ip in RATE_LIMIT:
        RATE_LIMIT[ip] = [t for t in RATE_LIMIT[ip] if now - t < WINDOW]
        if not RATE_LIMIT[ip]:
            del RATE_LIMIT[ip]
            
    RATE_LIMIT.setdefault(ip, [])
    if len(RATE_LIMIT.get(ip, [])) >= LIMIT:
        return False
    RATE_LIMIT[ip].append(now)
    return True

# ================= 3. LOAD C∆† S·ªû D·ªÆ LI·ªÜU (LU·∫¨T + X√É GIAO) =================
def load_faiss_db(index_file, pkl_file):
    try:
        if os.path.exists(index_file) and os.path.exists(pkl_file):
            index = faiss.read_index(index_file)
            with open(pkl_file, "rb") as f:
                docs = pickle.load(f)
            print(f"‚úÖ ƒê√£ t·∫£i DB: {index_file} ({len(docs)} docs)")
            return index, docs
        else:
            print(f"‚ö†Ô∏è Kh√¥ng t√¨m th·∫•y file: {index_file}")
            return None, None
    except Exception as e:
        print(f"‚ùå L·ªói t·∫£i DB {index_file}: {e}")
        return None, None

# Load c·∫£ 2 DB
index_luat, docs_luat = load_faiss_db("luat_vn.index", "luat_vn.pkl")
index_social, docs_social = load_faiss_db("xa_giao.index", "xa_giao.pkl")

# ================= 4. C√ÅC H√ÄM X·ª¨ L√ù T√åM KI·∫æM (CORE LOGIC) =================

# H√†m t·∫°o Vector t·ª´ c√¢u h·ªèi (D√πng OpenAI text-embedding-3-small)
async def get_embedding_async(text):
    if not openai_client: return None
    try:
        resp = await openai_client.embeddings.create(
            input=text,
            model="text-embedding-3-small"
        )
        # Chuy·ªÉn th√†nh numpy array float32 cho FAISS
        vec = np.array([resp.data[0].embedding]).astype('float32')
        faiss.normalize_L2(vec) # Chu·∫©n h√≥a vector
        return vec
    except Exception as e:
        print(f"‚ùå L·ªói Embedding: {e}")
        return None

# H√†m t√¨m ki·∫øm trong index c·ª• th·ªÉ
def search_index(index, docs, vector, top_k=3, threshold=0.0):
    if not index or not docs or vector is None:
        return []
    
    # Search
    scores, indices = index.search(vector, top_k)
    results = []
    
    # L·ªçc k·∫øt qu·∫£ theo ng∆∞·ª°ng (threshold)
    for i, score in enumerate(scores[0]):
        if score >= threshold:
            idx = indices[0][i]
            if 0 <= idx < len(docs):
                results.append(docs[idx])
    return results

# H√†m T√¨m ki·∫øm H·ªón h·ª£p (Lu·∫≠t + X√£ giao + Web)
async def hybrid_search(query):
    context_parts = []
    
    # 1. T·∫°o vector cho c√¢u h·ªèi
    q_vec = await get_embedding_async(query)

    # 2. T√¨m trong DB X√É GIAO (Ng∆∞·ª°ng cao ƒë·ªÉ tr√°nh nh·∫ßm)
    # N·∫øu c√¢u h·ªèi kh·ªõp > 45% v·ªõi c√¢u x√£ giao th√¨ l·∫•y
    social_res = search_index(index_social, docs_social, q_vec, top_k=2, threshold=0.45)
    if social_res:
        context_parts.append("[K·ªäCH B·∫¢N X√É GIAO/GIAO TI·∫æP]:\n" + "\n".join(social_res))

    # 3. T√¨m trong DB LU·∫¨T (Ng∆∞·ª°ng v·ª´a ph·∫£i)
    law_res = search_index(index_luat, docs_luat, q_vec, top_k=5, threshold=0.35)
    if law_res:
        context_parts.append("[D·ªÆ LI·ªÜU LU·∫¨T & NGH·ªä ƒê·ªäNH]:\n" + "\n".join(law_res))

    # 4. T√¨m ki·∫øm Internet (DuckDuckGo) - Ch·ªâ ch·∫°y khi kh√¥ng t√¨m th·∫•y lu·∫≠t trong DB
    # Ho·∫∑c lu√¥n ch·∫°y ƒë·ªÉ b·ªï sung tin t·ª©c m·ªõi (t√πy ch·ªçn)
    if not law_res: 
        try:
            ddg_res = await AsyncDDGS().text(
                f"{query} lu·∫≠t giao th√¥ng Vi·ªát Nam 2025",
                max_results=2,
                region="vn-vn"
            )
            if ddg_res:
                web_text = "\n".join([r['body'] for r in ddg_res])
                context_parts.append("[TH√îNG TIN INTERNET (THAM KH·∫¢O)]:\n" + web_text)
        except Exception:
            pass # L·ªói web th√¨ b·ªè qua

    return "\n\n---\n\n".join(context_parts)

# ================= 5. G·ªåI AI (GEMINI -> FALLBACK GPT) =================

# G·ªçi Google Gemini (Async)
async def call_gemini(api_key, prompt):
    genai.configure(api_key=api_key)
    # D√πng 1.5-flash cho nhanh v√† ·ªïn ƒë·ªãnh
    model = genai.GenerativeModel("gemini-2.5-flash")
    response = await model.generate_content_async(prompt)
    return response.text

# G·ªçi OpenAI GPT (Async) - D√πng l√†m Fallback
async def call_gpt_fallback(prompt):
    if not openai_client:
        raise RuntimeError("Kh√¥ng c√≥ OpenAI Key ƒë·ªÉ ch·∫°y Fallback")
    
    response = await openai_client.chat.completions.create(
        model="gpt-4o-mini", # R·∫ª v√† nhanh
        messages=[
            {"role": "system", "content": "B·∫°n l√† chuy√™n gia Lu·∫≠t Giao th√¥ng VN."},
            {"role": "user", "content": prompt}
        ],
        temperature=0.3
    )
    return response.choices[0].message.content

# ================= 6. API ENDPOINT =================
class ChatRequest(BaseModel):
    prompt: str

@app.post("/api/process")
async def process_data(req: Request, body: ChatRequest):
    # 1. Check Rate Limit
    ip = req.client.host
    if not check_rate_limit(ip):
        raise HTTPException(429, "B·∫°n g·ª≠i qu√° nhi·ªÅu y√™u c·∫ßu. Vui l√≤ng ƒë·ª£i 1 ph√∫t.")

    user_input = body.prompt.strip()
    if not user_input:
        return {"answer": "B·∫°n ch∆∞a nh·∫≠p c√¢u h·ªèi n√†o c·∫£! üòÖ"}

    # 2. T√¨m ki·∫øm d·ªØ li·ªáu (Search)
    context = await hybrid_search(user_input)

    # 3. T·∫°o Prompt
    source_warning = ""
    if not context:
        source_warning = "\n‚ö†Ô∏è *L∆∞u √Ω: Kh√¥ng t√¨m th·∫•y d·ªØ li·ªáu trong th∆∞ vi·ªán. C√¢u tr·∫£ l·ªùi d·ª±a tr√™n ki·∫øn th·ª©c t·ªïng h·ª£p c·ªßa AI.*"
        
    system_instruction = """
    VAI TR√í: B·∫°n l√† Tr·ª£ l√Ω AI C·ªë v·∫•n Ph√°p lu·∫≠t Giao th√¥ng Vi·ªát Nam & B·∫°n ƒë∆∞·ªùng tin c·∫≠y.
    
    NHI·ªÜM V·ª§:
    1. N·∫øu l√† c√¢u h·ªèi X√É GIAO (Ch√†o h·ªèi, tr√™u ƒë√πa, h·ªèi t√™n...):
       - Tr·∫£ l·ªùi th√¢n thi·ªán, h√†i h∆∞·ªõc, ng·∫Øn g·ªçn.
       
    2. N·∫øu l√† c√¢u h·ªèi LU·∫¨T/KI·∫æN TH·ª®C:
       - D·ª±a tuy·ªát ƒë·ªëi v√†o [NG·ªÆ C·∫¢NH THAM KH·∫¢O] b√™n d∆∞·ªõi.
       - Tr√≠ch d·∫´n Ngh·ªã ƒë·ªãnh 100/2019 ho·∫∑c 123/2021 ho·∫∑c 168/2024.
       - N√™u r√µ: M·ª©c ph·∫°t ti·ªÅn (In ƒë·∫≠m) v√† H√¨nh ph·∫°t b·ªï sung (T∆∞·ªõc b·∫±ng, giam xe...).
       - Tr√¨nh b√†y d·∫°ng danh s√°ch (Bullet points) d·ªÖ ƒë·ªçc.
    
    3. NGUY√äN T·∫ÆC:
       - Kh√¥ng b·ªãa ƒë·∫∑t m·ª©c ph·∫°t.
       - Lu√¥n d√πng Emoji (üöó, üëÆ, üí∞) ƒë·ªÉ sinh ƒë·ªông.
    """

    final_prompt = f"""
    [SYSTEM]
    {system_instruction}

    [NG·ªÆ C·∫¢NH THAM KH·∫¢O T·ª™ DATABASE & INTERNET]
    {context if context else "Kh√¥ng c√≥ d·ªØ li·ªáu c·ª• th·ªÉ."}

    [C√ÇU H·ªéI NG∆Ø·ªúI D√ôNG]
    {user_input}

    [TR·∫¢ L·ªúI]
    """

    # 4. CHI·∫æN THU·∫¨T G·ªåI AI: GEMINI XOAY V√íNG -> GPT FALLBACK
    
    # --- GIAI ƒêO·∫†N 1: Th·ª≠ t·∫•t c·∫£ key Gemini ---
    for idx, key in enumerate(GOOGLE_KEYS):
        try:
            answer = await call_gemini(key, final_prompt)
            return {
                "answer": answer + source_warning,
                "model": "gemini",
                "key_used": idx, # ƒê·ªÉ debug xem ƒëang d√πng key n√†o
                "status": "success"
            }
        except Exception as e:
            print(f"‚ö†Ô∏è Gemini Key {idx} l·ªói: {e}. ƒêang th·ª≠ key ti·∫øp theo...")
            continue # Th·ª≠ key k·∫ø ti·∫øp

    # --- GIAI ƒêO·∫†N 2: N·∫øu t·∫•t c·∫£ Key Gemini ƒë·ªÅu l·ªói -> D√πng GPT ---
    print("üö® T·∫§T C·∫¢ KEY GEMINI ƒê·ªÄU L·ªñI. CHUY·ªÇN SANG GPT FALLBACK!")
    try:
        answer = await call_gpt_fallback(final_prompt)
        return {
            "answer": answer + source_warning,
            "model": "gpt-fallback",
            "status": "success"
        }
    except Exception as e:
        print(f"‚ùå GPT Fallback c≈©ng l·ªói: {e}")
        return {
            "answer": "H·ªá th·ªëng ƒëang qu√° t·∫£i v√† b·∫£o tr√¨. B·∫°n vui l√≤ng th·ª≠ l·∫°i sau 1 ph√∫t nh√©! üòî",
            "status": "error"
        }

# ================= 7. HEALTH CHECK =================
@app.get("/")
def health_check():
    return {
        "status": "online",
        "mode": "Hybrid (Law + Social)",
        "gemini_keys": len(GOOGLE_KEYS),
        "gpt_ready": bool(OPENAI_API_KEY),
        "db_law": index_luat is not None,
        "db_social": index_social is not None
    }
