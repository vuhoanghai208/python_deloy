from fastapi import FastAPI, HTTPException
from pydantic import BaseModel
from fastapi.middleware.cors import CORSMiddleware
import faiss
import pickle
import numpy as np
import os
import google.generativeai as genai
from openai import OpenAI
import time

app = FastAPI()

# 1. Cáº¥u hÃ¬nh CORS
app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"], 
    allow_methods=["*"],
    allow_headers=["*"],
)

# 2. Cáº¤U HÃŒNH API KEYS (HYBRID)

# A. Key OpenAI (DÃ¹ng Ä‘á»ƒ TÃŒM KIáº¾M - Embedding)
OPENAI_API_KEY = os.getenv("OPENAI_API_KEY")
openai_client = OpenAI(api_key=OPENAI_API_KEY)

# B. Key Google (DÃ¹ng Ä‘á»ƒ TRáº¢ Lá»œI - Chat Generative)
GOOGLE_KEYS_STR = os.getenv("GOOGLE_API_KEYS", "")
GOOGLE_KEYS = [k.strip() for k in GOOGLE_KEYS_STR.split(",") if k.strip()]
key_index = 0

def get_current_google_key():
    global key_index
    if not GOOGLE_KEYS: return None
    return GOOGLE_KEYS[key_index % len(GOOGLE_KEYS)]

# 3. Health Check
@app.get("/")
def read_root():
    return {"status": "Hybrid Server is running (OpenAI Search + Gemini Chat)"}

# 4. Load Database (LÆ°u Ã½: Pháº£i lÃ  DB Ä‘Æ°á»£c táº¡o báº±ng OpenAI text-embedding-3-small)
print("ğŸ“¥ Äang táº£i cÆ¡ sá»Ÿ dá»¯ liá»‡u luáº­t...")
index = None
documents = None

try:
    if os.path.exists("luat_vn.index") and os.path.exists("luat_vn.pkl"):
        index = faiss.read_index("luat_vn.index")
        with open("luat_vn.pkl", "rb") as f:
            documents = pickle.load(f)
        print(f"âœ… ÄÃ£ táº£i xong! Tá»•ng cá»™ng {len(documents)} Ä‘oáº¡n luáº­t.")
    else:
        print("âš ï¸ Lá»—i: KhÃ´ng tÃ¬m tháº¥y file dá»¯ liá»‡u. HÃ£y cháº¡y build_db_openai.py!")
except Exception as e:
    print(f"âŒ Lá»—i khi táº£i DB: {e}")

# 5. HÃ€M TÃŒM KIáº¾M (DÃ¹ng OpenAI Embedding)
def vector_search(query):
    if not index or not documents:
        print("âŒ Lá»—i: DB chÆ°a Ä‘Æ°á»£c load.")
        return ""

    try:
        # Gá»i OpenAI Ä‘á»ƒ mÃ£ hÃ³a cÃ¢u há»i
        # LÆ°u Ã½: Model nÃ y pháº£i KHá»šP vá»›i model lÃºc báº¡n cháº¡y build_db.py
        response = openai_client.embeddings.create(
            input=query,
            model="text-embedding-3-small"
        )
        
        # Láº¥y vector
        q_vec = np.array([response.data[0].embedding]).astype('float32')
        faiss.normalize_L2(q_vec) 
        
        # TÃ¬m kiáº¿m trong FAISS
        scores, indices = index.search(q_vec, 5)
        
        relevant_docs = []
        print(f"ğŸ” Káº¿t quáº£ tÃ¬m kiáº¿m cho: '{query}'")
        for i, score in enumerate(scores[0]):
            if score >= 0.35: # NgÆ°á»¡ng lá»c
                print(f"   - Äoáº¡n {indices[0][i]} (Score: {score:.4f})")
                relevant_docs.append(documents[indices[0][i]])
        
        if relevant_docs:
            return "\n---\n".join(relevant_docs)
        else:
            print("   -> KhÃ´ng tÃ¬m tháº¥y Ä‘oáº¡n nÃ o khá»›p > 0.35")
            return ""
            
    except Exception as e:
        # ÄÃ‚Y LÃ€ CHá»– IN RA Lá»–I TÃŒM KIáº¾M Cá»¦A Báº N
        print(f"âŒ Lá»–I TÃŒM KIáº¾M (OpenAI Embedding): {e}")
        return ""

# 6. API Xá»­ lÃ½ Chat
class ChatRequest(BaseModel):
    prompt: str

@app.post("/api/process")
async def process_data(request: ChatRequest):
    user_input = request.prompt
    
    # --- BÆ¯á»šC 1: TÃŒM KIáº¾M (DÃ¹ng OpenAI) ---
    context = vector_search(user_input)
    
    # --- BÆ¯á»šC 2: Táº O PROMPT ---
    # --- TRONG FILE main.py ---

    # --- BÆ¯á»šC 2: Táº O PROMPT THÃ”NG MINH ---
    
    # Ká»‹ch báº£n 1: CÃ³ dá»¯ liá»‡u chÃ­nh xÃ¡c tá»« DB
    if context:
        source_info = f"Dá»±a trÃªn tÃ i liá»‡u luáº­t: \n{context}"
        guidance = "HÃ£y tráº£ lá»i CHÃNH XÃC dá»±a trÃªn thÃ´ng tin trÃªn."
    else:
        # Ká»‹ch báº£n 2: KhÃ´ng tÃ¬m tháº¥y trong DB -> DÃ¹ng kiáº¿n thá»©c rá»™ng cá»§a AI (Hybrid)
        source_info = "KhÃ´ng tÃ¬m tháº¥y thÃ´ng tin cá»¥ thá»ƒ trong bá»™ dá»¯ liá»‡u luáº­t hiá»‡n táº¡i."
        guidance = """
        HÃ£y váº­n dá»¥ng kiáº¿n thá»©c rá»™ng cá»§a báº¡n vá» Luáº­t Giao thÃ´ng Ä‘Æ°á»ng bá»™ Viá»‡t Nam (Nghá»‹ Ä‘á»‹nh 100, 123, 168) Ä‘á»ƒ tráº£ lá»i.
        TUY NHIÃŠN: Pháº£i thÃªm cÃ¢u cáº£nh bÃ¡o nhá» á»Ÿ cuá»‘i: "ThÃ´ng tin nÃ y dá»±a trÃªn kiáº¿n thá»©c tá»•ng há»£p, báº¡n nÃªn tra cá»©u vÄƒn báº£n gá»‘c Ä‘á»ƒ Ä‘á»‘i chiáº¿u."
        """

    system_prompt = f"""
    Báº¡n lÃ  Trá»£ lÃ½ AI ThÃ´ng minh vá» An toÃ n Giao thÃ´ng Viá»‡t Nam.
    Phong cÃ¡ch cá»§a báº¡n: ThÃ¢n thiá»‡n, chuyÃªn nghiá»‡p, trÃ¬nh bÃ y Ä‘áº¹p máº¯t, dá»… hiá»ƒu.

    Dá»® LIá»†U THAM KHáº¢O:
    ---------------------
    {source_info}
    ---------------------

    QUY Táº®C TRÃŒNH BÃ€Y (Báº®T BUá»˜C TUÃ‚N THá»¦):
    1. **Äá»ŠNH Dáº NG:**
       - **TUYá»†T Äá»I KHÃ”NG** dÃ¹ng dáº¥u sao (*) á»Ÿ Ä‘áº§u dÃ²ng danh sÃ¡ch. NÃ³ gÃ¢y xáº¥u giao diá»‡n.
       - HÃ£y dÃ¹ng dáº¥u gáº¡ch ngang (-) hoáº·c sá»‘ thá»© tá»± (1., 2.) cho cÃ¡c danh sÃ¡ch.
       - DÃ¹ng **In Ä‘áº­m** (bá»c trong 2 dáº¥u sao) cho: Sá»‘ tiá»n pháº¡t, TÃªn lá»—i vi pháº¡m, CÃ¡c tá»« khÃ³a quan trá»ng.
    
    2. **Bá» Cá»¤C & KHOáº¢NG CÃCH:**
       - Giá»¯a cÃ¡c Ã½ chÃ­nh pháº£i cÃ³ **má»™t dÃ²ng trá»‘ng** Ä‘á»ƒ táº¡o Ä‘á»™ thoÃ¡ng.
       - KhÃ´ng viáº¿t má»™t Ä‘oáº¡n vÄƒn quÃ¡ dÃ i (trÃªn 5 dÃ²ng). HÃ£y ngáº¯t nhá» ra.

    3. **EMOJI & SINH Äá»˜NG:**
       - LuÃ´n thÃªm Emoji phÃ¹ há»£p (ğŸš—, ğŸ›µ, ğŸ›‘, ğŸ’°, ğŸ‘®, âš ï¸, âœ…) vÃ o Ä‘áº§u cÃ¡c Ã½ chÃ­nh hoáº·c tiÃªu Ä‘á».
    
    4. **Ná»˜I DUNG:**
       - Äi tháº³ng vÃ o váº¥n Ä‘á». KhÃ´ng vÃ²ng vo.
       - Náº¿u cÃ¢u há»i vá» xá»­ pháº¡t: **PHáº¢I** ghi rÃµ con sá»‘ cá»¥ thá»ƒ (VÃ­ dá»¥: **2.000.000Ä‘ - 3.000.000Ä‘**).
       - {guidance}
    """
    
    final_prompt = f"NgÆ°á»i dÃ¹ng: {user_input}"

    # --- BÆ¯á»šC 3: TRáº¢ Lá»œI (DÃ¹ng Google Gemini - Äá»ƒ tiáº¿t kiá»‡m tiá»n) ---
    global key_index
    for i in range(len(GOOGLE_KEYS)):
        try:
            current_key = get_current_google_key()
            genai.configure(api_key=current_key)
            
            # DÃ¹ng model 1.5-flash (Báº£n á»•n Ä‘á»‹nh nháº¥t hiá»‡n táº¡i)
            model = genai.GenerativeModel('gemini-2.5-flash')
            
            response = model.generate_content(f"{system_prompt}\n\n{final_prompt}")
            
            # Tráº£ vá» káº¿t quáº£ JSON chuáº©n cho Frontend
            return {"answer": response.text}
            
        except Exception as e:
            print(f"âš ï¸ Lá»—i Gemini (Key {i}): {e}")
            key_index += 1
            time.sleep(0.5)
            
    # Náº¿u táº¥t cáº£ Ä‘á»u lá»—i
    return {"answer": "Há»‡ thá»‘ng Ä‘ang quÃ¡ táº£i hoáº·c gáº·p sá»± cá»‘ káº¿t ná»‘i. Vui lÃ²ng thá»­ láº¡i sau."}
