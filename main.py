from fastapi import FastAPI, HTTPException
from pydantic import BaseModel
from fastapi.middleware.cors import CORSMiddleware
import faiss
import pickle
import numpy as np
import os
import google.generativeai as genai
from openai import OpenAI
import time

app = FastAPI()

# 1. C·∫•u h√¨nh CORS
app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"], 
    allow_methods=["*"],
    allow_headers=["*"],
)

# 2. C·∫§U H√åNH API KEYS (HYBRID)

# A. Key OpenAI (D√πng ƒë·ªÉ T√åM KI·∫æM - Embedding)
OPENAI_API_KEY = os.getenv("OPENAI_API_KEY")
openai_client = OpenAI(api_key=OPENAI_API_KEY)

# B. Key Google (D√πng ƒë·ªÉ TR·∫¢ L·ªúI - Chat Generative)
GOOGLE_KEYS_STR = os.getenv("GOOGLE_API_KEYS", "")
GOOGLE_KEYS = [k.strip() for k in GOOGLE_KEYS_STR.split(",") if k.strip()]
key_index = 0

def get_current_google_key():
    global key_index
    if not GOOGLE_KEYS: return None
    return GOOGLE_KEYS[key_index % len(GOOGLE_KEYS)]

# 3. Health Check
@app.get("/")
def read_root():
    return {"status": "Hybrid Server is running (OpenAI Search + Gemini Chat)"}

# 4. Load Database (L∆∞u √Ω: Ph·∫£i l√† DB ƒë∆∞·ª£c t·∫°o b·∫±ng OpenAI text-embedding-3-small)
print("üì• ƒêang t·∫£i c∆° s·ªü d·ªØ li·ªáu lu·∫≠t...")
index = None
documents = None

try:
    if os.path.exists("luat_vn.index") and os.path.exists("luat_vn.pkl"):
        index = faiss.read_index("luat_vn.index")
        with open("luat_vn.pkl", "rb") as f:
            documents = pickle.load(f)
        print(f"‚úÖ ƒê√£ t·∫£i xong! T·ªïng c·ªông {len(documents)} ƒëo·∫°n lu·∫≠t.")
    else:
        print("‚ö†Ô∏è L·ªói: Kh√¥ng t√¨m th·∫•y file d·ªØ li·ªáu. H√£y ch·∫°y build_db_openai.py!")
except Exception as e:
    print(f"‚ùå L·ªói khi t·∫£i DB: {e}")

# 5. H√ÄM T√åM KI·∫æM (D√πng OpenAI Embedding)
def vector_search(query):
    if not index or not documents:
        print("‚ùå L·ªói: DB ch∆∞a ƒë∆∞·ª£c load.")
        return ""

    try:
        # G·ªçi OpenAI ƒë·ªÉ m√£ h√≥a c√¢u h·ªèi
        # L∆∞u √Ω: Model n√†y ph·∫£i KH·ªöP v·ªõi model l√∫c b·∫°n ch·∫°y build_db.py
        response = openai_client.embeddings.create(
            input=query,
            model="text-embedding-3-small"
        )
        
        # L·∫•y vector
        q_vec = np.array([response.data[0].embedding]).astype('float32')
        faiss.normalize_L2(q_vec) 
        
        # T√¨m ki·∫øm trong FAISS
        scores, indices = index.search(q_vec, 5)
        
        relevant_docs = []
        print(f"üîç K·∫øt qu·∫£ t√¨m ki·∫øm cho: '{query}'")
        for i, score in enumerate(scores[0]):
            if score >= 0.35: # Ng∆∞·ª°ng l·ªçc
                print(f"   - ƒêo·∫°n {indices[0][i]} (Score: {score:.4f})")
                relevant_docs.append(documents[indices[0][i]])
        
        if relevant_docs:
            return "\n---\n".join(relevant_docs)
        else:
            print("   -> Kh√¥ng t√¨m th·∫•y ƒëo·∫°n n√†o kh·ªõp > 0.35")
            return ""
            
    except Exception as e:
        # ƒê√ÇY L√Ä CH·ªñ IN RA L·ªñI T√åM KI·∫æM C·ª¶A B·∫†N
        print(f"‚ùå L·ªñI T√åM KI·∫æM (OpenAI Embedding): {e}")
        return ""

# 6. API X·ª≠ l√Ω Chat
class ChatRequest(BaseModel):
    prompt: str

@app.post("/api/process")
async def process_data(request: ChatRequest):
    user_input = request.prompt
    
    # --- B∆Ø·ªöC 1: T√åM KI·∫æM (D√πng OpenAI) ---
    context = vector_search(user_input)
    
    # --- B∆Ø·ªöC 2: T·∫†O PROMPT ---
    if context:
        system_prompt = f"""
        B·∫°n l√† Tr·ª£ l√Ω Ph√°p lu·∫≠t Giao th√¥ng Vi·ªát Nam (Ngh·ªã ƒë·ªãnh 168/2024).
        D∆∞·ªõi ƒë√¢y l√† th√¥ng tin tr√≠ch xu·∫•t t·ª´ vƒÉn b·∫£n lu·∫≠t:
        ---------------------
        {context}
        ---------------------
        Y√äU C·∫¶U:
        1. CH·ªà s·ª≠ d·ª•ng th√¥ng tin tr√™n ƒë·ªÉ tr·∫£ l·ªùi.
        2. N·∫øu c√≥ m·ª©c ph·∫°t ti·ªÅn, h√£y ghi r√µ con s·ªë.
        3. Tr·∫£ l·ªùi ng·∫Øn g·ªçn, s√∫c t√≠ch, th√¢n thi·ªán.
        """
        final_prompt = f"Ng∆∞·ªùi d√πng h·ªèi: {user_input}"
    else:
        # N·∫øu kh√¥ng t√¨m th·∫•y lu·∫≠t, v·∫´n cho ph√©p Gemini ch√©m gi√≥ (nh∆∞ng c·∫£nh b√°o)
        # Ho·∫∑c tr·∫£ l·ªùi kh√©o l√©o nh∆∞ file soucre b·∫°n g·ª≠i
        system_prompt = """
        B·∫°n l√† Tr·ª£ l√Ω Giao th√¥ng.
        Ng∆∞·ªùi d√πng ƒëang h·ªèi m·ªôt c√¢u m√† trong d·ªØ li·ªáu lu·∫≠t hi·ªán t·∫°i KH√îNG t√¨m th·∫•y.
        H√£y tr·∫£ l·ªùi d·ª±a tr√™n ki·∫øn th·ª©c chung c·ªßa b·∫°n nh∆∞ng ph·∫£i th√™m c√¢u c·∫£nh b√°o: "Th√¥ng tin n√†y ch·ªâ mang t√≠nh tham kh·∫£o do ch∆∞a t√¨m th·∫•y trong vƒÉn b·∫£n lu·∫≠t ƒë∆∞·ª£c cung c·∫•p."
        """
        final_prompt = f"Ng∆∞·ªùi d√πng h·ªèi: {user_input}"

    # --- B∆Ø·ªöC 3: TR·∫¢ L·ªúI (D√πng Google Gemini - ƒê·ªÉ ti·∫øt ki·ªám ti·ªÅn) ---
    global key_index
    for i in range(len(GOOGLE_KEYS)):
        try:
            current_key = get_current_google_key()
            genai.configure(api_key=current_key)
            
            # D√πng model 1.5-flash (B·∫£n ·ªïn ƒë·ªãnh nh·∫•t hi·ªán t·∫°i)
            model = genai.GenerativeModel('gemini-1.5-flash')
            
            response = model.generate_content(f"{system_prompt}\n\n{final_prompt}")
            
            # Tr·∫£ v·ªÅ k·∫øt qu·∫£ JSON chu·∫©n cho Frontend
            return {"answer": response.text}
            
        except Exception as e:
            print(f"‚ö†Ô∏è L·ªói Gemini (Key {i}): {e}")
            key_index += 1
            time.sleep(0.5)
            
    # N·∫øu t·∫•t c·∫£ ƒë·ªÅu l·ªói
    return {"answer": "H·ªá th·ªëng ƒëang qu√° t·∫£i ho·∫∑c g·∫∑p s·ª± c·ªë k·∫øt n·ªëi. Vui l√≤ng th·ª≠ l·∫°i sau."}
